{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Types and Missing Values\n",
    "\n",
    "Each column of data in a pandas DataFrame has a data type. This is a very similar concept to types in Python. Just like every Python object has a type, every column has a data type. pandas has a large number of data types available for each column. This chapter focuses only on the most common data types and provides a brief summary of each one. For extensive coverage of each and every data type, see chapter **Changing Data Types** in the **Essential Commands** part.\n",
    "\n",
    "### Most Common Data Types\n",
    "The following are the most common data types that appear frequently in DataFrames. \n",
    "\n",
    "* **boolean** - only two possible values, `True` and `False`\n",
    "* **integer** - whole numbers without decimals\n",
    "* **float** - numbers with decimals\n",
    "* **object** - typically strings, but may contain any object\n",
    "* **datetime** - a specific date and time with nanosecond precision\n",
    "\n",
    "### More on the object data type\n",
    "The object data type is the most confusing and deserves a longer discussion. Each value in an object column can be **any** Python object. This means object columns can contain integers, floats, or even complex types such as lists or dictionaries. Anything can be contained in object columns.  But, nearly all of the time, object data type columns contain **strings**. When you see that a column is an object data type, you should expect the values to be strings. pandas, unfortunately, does not provide its users with a specific data type for strings so if you do have strings in your columns, the data type will be object.\n",
    "\n",
    "### The importance of knowing the data type\n",
    "\n",
    "Knowing the data type of each column of your pandas DataFrame is very important. The main reason for this is that every value in each column will be of the same type. For instance, if you select a single value from a column that has an integer data type, then you are guaranteed that this value is also an integer.  Knowing the data type of a column is one of the most fundamental pieces of knowledge of your DataFrame.\n",
    "\n",
    "### A major exception with the object data type\n",
    "\n",
    "The object data type, is unfortunately, an exception to the information in the previous section. Although columns that have object data type are typically strings, there is no guarantee that each value will be a string. You could very well have an integer, list, or even another DataFrame as a value in the same object column.\n",
    "\n",
    "## Missing Value Representation\n",
    "\n",
    "### `NaN`,  `None`, and `NaT`\n",
    "pandas represents missing values differently based on the data type of the column.\n",
    "\n",
    "* `NaN` stands for not a number and is technically a float data type\n",
    "* `None` is the literal Python object `None` and will only be found in object columns\n",
    "* `NaT` stands for not a time and is used for missing values in datetime columns\n",
    "\n",
    "### Missing values for each data type\n",
    "\n",
    "* **boolean and integer** - No representation for missing values exist for boolean and integer columns. This is an unfortunate limitation, though recently pandas has created an entire new integer data type that does have `NaN`s available.\n",
    "\n",
    "* **floats** - Uses only `NaN` as the missing value.  \n",
    "\n",
    "* **object** - Columns of object data type can contain any Python object so all three of the missing value representation may appear in these columns but typically they will be either `NaN` or `None`.\n",
    "\n",
    "* **datetime** - Uses only `NaT` as the missing value.\n",
    "\n",
    "### Missing values in boolean and integer columns\n",
    "Knowing that a column is either a boolean or integer column guarantees that there are no missing values in that column as pandas does not allow for it. If, for instance, you would like to place a missing value in a boolean or integer column, then pandas will convert the entire column to float as it is a numerical data type with a missing value representation. When booleans are converted to floats, `False` becomes 0 and `True` becomes 1.\n",
    "\n",
    "### Integer NaN update for pandas 0.24\n",
    "\n",
    "With the release of pandas version 0.24 (February 2019), missing value representation was made possible for a special kind of integer data type called **Int64Dtype**. There is still no missing value representation for the default integer data type. \n",
    "\n",
    "## Finding the data types of each column\n",
    "The `dtypes` DataFrame attribute (NOT a method) returns the data type of each column. Let's get the data types of our `bikes` DataFrame. Note that the returned data is a Series with the column names now in the index and the data type as the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "bikes = pd.read_csv('../data/bikes.csv')\n",
    "bikes.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why are `starttime` and `stoptime` object data types?\n",
    "If you look at the output of the `bikes` DataFrame, it's apparent that both the `starttime` and `stoptime` columns are datetimes, but the output from `dtypes` states that they are objects. The `read_csv` function requires that you provide a list of columns that are datetimes to the `parse_dates` parameter, otherwise it will read them in as strings. Let's reread the data using the `parse_dates` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes = pd.read_csv('../data/bikes.csv', parse_dates=['starttime', 'stoptime'])\n",
    "bikes.dtypes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are all those 64's at the end of the data types?\n",
    "Booleans, integers, floats, and datetimes all use a particular amount of memory for each of their values. The memory is measured in **bits**. The number of bits used for each value is the number appended to the end of the data type name. For instance, integers can be either 8, 16, 32, or 64 bits while floats can be 16, 32, 64, or 128. A 128-bit float column will show up as `float128`. \n",
    "\n",
    "Technically a `float128` is a different data type than a `float64` but generally you will not have to worry about such a distinction as the operations between different float columns will be the same. \n",
    "\n",
    "**Booleans** are always stored as 8-bits. There is no set bit size for values in an **object** column as each value can be of any size.\n",
    "\n",
    "## Getting more metadata\n",
    "Metadata can be defined as data on the data. The data type of each column is an example of **metadata**. The number of rows and columns is another piece of metadata. We find this with the `shape` attribute, which returns a tuple of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total number of values with the `size` attribute\n",
    "The `size` attribute returns the total number of values (the number of columns multiplied by the number of rows) in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data types plus more with the `info` method\n",
    "The `info` DataFrame method provides output similar to `dtypes`, but also shows the number of non-missing values in each column along with more info such as:  \n",
    "\n",
    "* Type of object (always a DataFrame)\n",
    "* The type of index and number of rows\n",
    "* The number of columns\n",
    "* The data types of each column and the number of non-missing (a.k.a non-null)\n",
    "* The frequency count of all data types\n",
    "* The total memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More data types\n",
    "There are several more data types available in pandas. An extensive and formal discussion on all data types is available in the chapter **Changing Data Types** from the **Essential Commands** part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "Use the `bikes` DataFrame for the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "<span  style=\"color:green; font-size:16px\">What type of object is returned from the `dtypes` attribute?</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "<span  style=\"color:green; font-size:16px\">What type of object is returned from the `shape` attribute?</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "<span style=\"color:green; font-size:16px\">What type of object is returned from the `info` method?</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "<span  style=\"color:green; font-size:16px\">The memory usage from the `info` method isn't correct when you have objects in your DataFrame. Read the docstrings from it and get the true memory usage.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
